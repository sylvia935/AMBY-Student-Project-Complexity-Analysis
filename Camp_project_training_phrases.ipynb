{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDbhx3iqMBxXjeM0JelD7j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sylvia935/AMBY-Student-Project-Complexity-Analysis/blob/main/Camp_project_training_phrases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import packages"
      ],
      "metadata": {
        "id": "l5I6LNRA5Uxn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdgIeByL5F_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258df21f-0ec6-4b03-9b4b-901c1ac15f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# import neccessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import itertools\n",
        "import collections\n",
        "from collections import Counter\n",
        "import regex as re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "import seaborn as sns\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Import data"
      ],
      "metadata": {
        "id": "KWrMaFH25aSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "#autenticating to google\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "import pandas as pd\n",
        "#defining my worksheet\n",
        "# worksheet = gc.open('Session B Per Project Aggregated Data').get_worksheet(1)\n",
        "worksheet = gc.open('Session A Per Project Aggregated Data').get_worksheet(1)\n",
        "#get_all_values gives a list of rows\n",
        "rows = worksheet.get_all_values()\n",
        "#Convert to a DataFrame \n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# add header\n",
        "new_header = df.iloc[0] #grab the first row for the header\n",
        "df = df[1:] #take the data less the header row\n",
        "df.columns = new_header #set the header row as the df header"
      ],
      "metadata": {
        "id": "PZIeD8fL5cNP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tp = df[['projectId','projectType','trainingPhrasesCount','responsesCount','trainingPhrasesContent', 'responsesContent']]\n",
        "# df_tp"
      ],
      "metadata": {
        "id": "PoOfKqYu5crk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(len(df_tp)): # i is the index of df_ms\n",
        "    # text is an utterance. \".strip('][').split(', ')\" is to convert string to list\n",
        "    tp_list =  df_tp.iloc[index]['trainingPhrasesContent'].strip('][').split(', ') \n",
        "    tp_length_total = 0\n",
        "    phrase_list = ''\n",
        "    # print(tp_list)\n",
        "    for phrase in tp_list:\n",
        "      # specific phrase length (word + punctuation)\n",
        "      # tokenized = nltk.word_tokenize(phrase[1:-1]) # [1:-1] is to remove the redundant ''\n",
        "      pure_phrase = phrase[1:-1]\n",
        "      phrase_list += pure_phrase + ' '\n",
        "      # comment this out to see the results\n",
        "    # print(phrase_list)\n",
        "    # print('word count: ', len(phrase_list.strip().split(\" \")))\n",
        "\n",
        "    df_tp.at[index,'phrase_list'] = phrase_list\n",
        "    df_tp.at[index,'TP_wordCount'] = len(phrase_list.strip().split(\" \"))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RHDCgd0r5co8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tp\n",
        "writer = pd.ExcelWriter('processed_TP_word_count.xlsx')\n",
        "df_tp.to_excel(writer)\n",
        "writer.save()"
      ],
      "metadata": {
        "id": "dGwRBXjZ5cl0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_b1McfvJ5chn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}